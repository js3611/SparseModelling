{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation for Dictionary Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Learn dictionary D such that (1)\n",
    "\n",
    "$$\\min_{D \\in \\mathcal{C}, A \\in \\mathbf{R}^{p x n}} \\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2} ||x_i - D \\alpha_i||_2^2 + \\lambda \\psi (\\alpha)$$\n",
    "\n",
    "or (2)\n",
    "\n",
    "$$\\min_{D \\in \\mathcal{C}, A \\in \\mathbf{R}^{p x n}} \\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2} ||x_i - D \\alpha_i||_2^2  \\quad s.t. \\quad \\psi (\\alpha) \\leq \\mu$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from l1_optimisations import coordinate_descent, soft_threshold\n",
    "from l0_optimisations import orthogonal_matching_pursuit as OMP, iterative_hard_thresholding, matching_pursuit as MP\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convex_projection(D):\n",
    "    return D / np.maximum(linalg.norm(D,axis=0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Alternates between \n",
    "\n",
    "1) Gradient Step \n",
    "\n",
    "2) Rescaling columns of $D$ (so $D \\in \\mathcal{C}$, set of matrices with $l_2$ norm of column is less than 1)\n",
    "\n",
    "Penalisation funciton $\\psi$ is either $l_1$-norm or smooth approximate sparsity-inducing penalty (i.e. $\\psi(\\alpha) = \\sum_{j=1}^p \\log(\\alpha[j]^2 + \\epsilon)$)\n",
    "\n",
    "Problem: how to choose the hyper parameters?\n",
    "\n",
    "(1) step size $\\eta_t  = \\eta / (t + t_0)^\\gamma$ \n",
    "\n",
    "(2) number of iteration $T$\n",
    "\n",
    "(3) Penalty Weight $\\lambda$\n",
    "\n",
    "(4) Batch Update \n",
    "\n",
    "(5) Whether to reuse A for initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, D0=None,lmbda=None,T=100,penalty='l1',lr=None, t0=None, gamma=None):\n",
    "    '''\n",
    "        Parameters:\n",
    "            X  - Observations, n x m matrix, where column vector is each data instance\n",
    "            D0 - Initial dictionary. It should be normalised by column\n",
    "                \n",
    "        Hyper-parameters:\n",
    "            Step size at t is calculated as: lr_t = lr / (t + t0)^gamma\n",
    "            \n",
    "            lr    - initial learning rate (ish)\n",
    "            t0    - controls the proportion of decay rate\n",
    "            gamma - controls the decay rate polynomially\n",
    "            \n",
    "        Returns:\n",
    "            D - dictionary\n",
    "    '''    \n",
    "    # Initialisation\n",
    "    m, n = X.shape       \n",
    "    lr = lr if lr else 0.1  # Learning rate / step size\n",
    "    t0 = t0 if t0 else 1\n",
    "    gamma = gamma if gamma else 1\n",
    "    \n",
    "    if not D0:        \n",
    "        D = random.rand(m,m) \n",
    "        D = D / linalg.norm(D, axis=0)      # normalise by column\n",
    "    else:\n",
    "        D = D0\n",
    "\n",
    "    if penalty == 'l1':\n",
    "        encode = coordinate_descent\n",
    "    else:\n",
    "        # TODO: other penalty functions\n",
    "        encode = coordinate_descent\n",
    "\n",
    "    if not lmbda:\n",
    "        lmbda = 0.1 # not sure how to pick a value here\n",
    "\n",
    "    A = np.zeros(p, n)\n",
    "        \n",
    "    # Main Loop\n",
    "    for t in xrange(T):\n",
    "        # compute sparse code for x_i picked at random\n",
    "        i = random.randint(0, n-1)\n",
    "        x = X[:,i]\n",
    "        alpha, _ = encode(D, x, lmbda)#, A[:, i])  # reuse old value of alpha to initialise?\n",
    "        #A[:, i] = alpha\n",
    "        \n",
    "        # perform one projected gradient descent\n",
    "        lr_t = lr / ((t0 + t) ** gamma)\n",
    "        unprojected_D = D - lr_t * (np.dot(np.dot(D,alpha) - x, alpha.T))\n",
    "        D = convex_projection(unprojected_D)\n",
    "        \n",
    "    return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?OMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Method of Optimal Directions (MOD): $l_0$-penalty\n",
    "\n",
    "Uses OMP to compute each value of alpha in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def method_of_optimal_directions(X, D0=None, k_max=None, T=100):\n",
    "    '''\n",
    "    Parameters:\n",
    "        X     - Observations, n x m matrix, where column vector is each data instance\n",
    "        D0    - Initial dictionary. It should be normalised by column\n",
    "        k_max - l0-sparsity constraint\n",
    "        T     - number of iteration\n",
    "\n",
    "    Returns:\n",
    "        D - dictionary\n",
    "        \n",
    "    '''\n",
    "    # Initialisation\n",
    "    m, n = X.shape       \n",
    "    \n",
    "    if not D0:\n",
    "        p = m\n",
    "        D = random.rand(m, p) \n",
    "        D = D / linalg.norm(D, axis=0)      # normalise by column\n",
    "    else:\n",
    "        D = D0\n",
    "        _, p = D.shape\n",
    "\n",
    "    if not lmbda:\n",
    "        lmbda = 0.1 # not sure how to pick a value here\n",
    "\n",
    "    if not k_max:\n",
    "        k_max = m / 10\n",
    "        \n",
    "    # main loop\n",
    "    for t in xrange(T):\n",
    "        # compute all alphas\n",
    "        A = np.zeros(p, n)\n",
    "        for i in xrange(n):\n",
    "            alpha, _ = OMP(D, X[:,i], k_max)\n",
    "            A[:,i] = alpha\n",
    "        unprojected_D = np.dot(X, np.dot(A.T, linalg.inv(np.dot(A, A.T))))\n",
    "        D = convex_projection(unprojected_D)\n",
    "    \n",
    "    return D\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Minimization: $l_1$-penalty\n",
    "\n",
    "Ideally requires homotopy but can use coordinate descent for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_update(D, X, A, T):\n",
    "    ## TODO ##\n",
    "    return D\n",
    "\n",
    "def bcd_update(D, X, A, T):\n",
    "    '''\n",
    "    Parameters:\n",
    "        T - number of iterations\n",
    "    Returns:\n",
    "        D - updated dictionary\n",
    "    \n",
    "    '''\n",
    "    ## TODO ##\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alternate_minimisation(X, D0=None, lmbda=None, mu=None, T=100, dict_update='Newton'):\n",
    "    '''\n",
    "    Parameters:\n",
    "        X     - Observations, n x m matrix, where column vector is each data instance\n",
    "        D0    - Initial dictionary. It should be normalised by column\n",
    "        lmbda - l1-sparsity constraint for Lasso formulation\n",
    "        mu    - hard constraint l1-sparsity\n",
    "        T     - number of iteration\n",
    "        dict_update - 'Newton' or 'Block' (Block Coordinate Descent)\n",
    "\n",
    "    Returns:\n",
    "        D - dictionary\n",
    "        \n",
    "    '''\n",
    "    # Initialisation\n",
    "    m, n = X.shape       \n",
    "    \n",
    "    if not D0:\n",
    "        p = m\n",
    "        D = random.rand(m, p) \n",
    "        D = D / linalg.norm(D, axis=0)      # normalise by column\n",
    "    else:\n",
    "        D = D0\n",
    "        _, p = D.shape\n",
    "\n",
    "    if not lmbda:\n",
    "        lmbda = 0.1 # not sure how to pick a value here\n",
    "        \n",
    "    if dict_update == 'Newton':\n",
    "        update_D = newton_update\n",
    "    else:\n",
    "        update_D = bcd_update\n",
    "        \n",
    "    # Main Loop    \n",
    "    for t in xrange(T):\n",
    "        A = np.zeros(p, n)\n",
    "        for i in xrange(n):\n",
    "            alpha, _ = coordinate_descent(D,X[:,i],lmbda)\n",
    "            A[:,i] = alpha\n",
    "        \n",
    "        # Update D by minimising objective function for D with fixed X and A.\n",
    "        D = update_D(D, X, A)\n",
    "        \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Coordinate Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block_coordinate_descent(X, D0=None, A0=None, lmbda=None, T=100):\n",
    "    '''\n",
    "    Parameters:\n",
    "        X     - Observations, n x m matrix, where column vector is each data instance\n",
    "        D0    - Initial dictionary. It should be normalised by column\n",
    "        A0    - sparse coefficients\n",
    "        lmbda - l1-sparsity constraint for Lasso formulation\n",
    "        mu    - hard constraint l1-sparsity\n",
    "        T     - number of iteration\n",
    "\n",
    "    Returns:\n",
    "        D - dictionary\n",
    "        \n",
    "    '''\n",
    "    # Initialisation\n",
    "    m, n = X.shape       \n",
    "    \n",
    "    if not D0:\n",
    "        p = m\n",
    "        D = random.rand(m, p) \n",
    "        D = D / linalg.norm(D, axis=0)      # normalise by column\n",
    "    else:\n",
    "        D = D0\n",
    "        _, p = D.shape\n",
    "        \n",
    "    if not A0:\n",
    "        A = np.zeros(p, n)\n",
    "    else:\n",
    "        A = A0\n",
    "\n",
    "    if not lmbda:\n",
    "        lmbda = 0.1 # not sure how to pick a value here\n",
    "        \n",
    "    if dict_update == 'Newton':\n",
    "        update_D = newton_update\n",
    "    else:\n",
    "        update_D = bcd_update\n",
    "        \n",
    "    # Main Loop    \n",
    "    for t in xrange(T):\n",
    "        \n",
    "        # update each row of A\n",
    "        for j in xrange(p):            \n",
    "            A_row_j = A[j,:] + np.dot(D[:,j], X - np.dot(D,A)) / linalg.norm(D[:,j])\n",
    "            A[j,:] = soft_threshold(A_row_j)\n",
    "                    \n",
    "        # Update D using block coordinate descent with iteration 1 ~ 5\n",
    "        \n",
    "        D = bcd_update(D, X, A, T=5)\n",
    "        \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-SVD\n",
    "\n",
    "Solves for $l_0$-regularised dictionary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_svd(X, D0=None, mu=None, T=100):\n",
    "    '''\n",
    "    Uses SVD to approximate each dictionary element\n",
    "    Parameters:\n",
    "        X   - data\n",
    "        D0  - initial dictionary\n",
    "        mu  - target sparsity\n",
    "        T   - number of iterations\n",
    "    Returns:\n",
    "        D - updated dictionary        \n",
    "    '''\n",
    "    m, n = X.shape\n",
    "    esp = 1e-6\n",
    "    \n",
    "    if not D0:\n",
    "        p = m\n",
    "        D = random.rand(m, p) \n",
    "        D = D / linalg.norm(D, axis=0)      # normalise by column\n",
    "    else:\n",
    "        _, p = D0.shape\n",
    "        D = D0\n",
    "\n",
    "    mu = mu if mu else p/10\n",
    "        \n",
    "    A = np.zeros([p, n])\n",
    "    \n",
    "    # Main loop\n",
    "    for t in xrange(T):\n",
    "        \n",
    "        # Compute A\n",
    "        for i in xrange(n):\n",
    "            A[:,i] = OMP(D, X[:,i], mu)[0]\n",
    "                    \n",
    "        # Compute D and A (nonzero coeffs of A)\n",
    "        for j in xrange(p):\n",
    "            # define set of indices of X which uses D[:,j] for sparse encoding\n",
    "            indices = [i for i, v in enumerate(A[j,:]) if abs(v) < eps]\n",
    "            \n",
    "            S = np.zeros([n, len(indices)])\n",
    "            for i, idx in enumerate(indices):\n",
    "                S[idx, i] = 1\n",
    "            \n",
    "            # error matrix\n",
    "            E = X - np.dot(D, A) + np.outer(D[:,j],A[j,:]) \n",
    "            \n",
    "            # Sparsify\n",
    "            E_sparse = np.dot(E, S) \n",
    "            \n",
    "            # compute the svd to get d and a\n",
    "            u, s, v = linalg.svd(E_sparse, compute_uv=True)\n",
    "\n",
    "            # update\n",
    "            D[:,j] = u[:,0] \n",
    "            A[j,indices] = s[0] * v[0,:]\n",
    "                    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
